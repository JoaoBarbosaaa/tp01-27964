# TP01 - Kettle (Pentaho Data Integration)

## ğŸ“˜ IdentificaÃ§Ã£o do Autor
**Nome:** JoÃ£o Barbosa  
**NÃºmero de Aluno:** 27964
**Unidade Curricular:** IntegraÃ§Ã£o de sistemas de informaÃ§Ã£o

**Ferramenta Utilizada:** Pentaho Data Integration (Kettle)

---

## ğŸ“‚ Estrutura do Projeto
tp01-27964/
â”‚
â”œâ”€â”€ README.md â†’ Ficheiro de descriÃ§Ã£o do projeto
â”‚
â”œâ”€â”€ doc/
â”‚ â””â”€â”€ 27964_27983_doc.pdf â†’ Documento explicativo do trabalho (anÃ¡lise e resultados)
â”‚
â”œâ”€â”€ dataint/
â”‚ â”œâ”€â”€ logs/ â†’ Ficheiros de logs gerados durante a execuÃ§Ã£o das transformaÃ§Ãµes/jobs
â”‚ â”œâ”€â”€ *.csv â†’ Ficheiros intermÃ©dios ou auxiliares utilizados no processo ETL
â”‚ â”œâ”€â”€ *.ktr â†’ TransformaÃ§Ãµes desenvolvidas no Kettle
â”‚ â””â”€â”€ *.kjb â†’ Jobs utilizados para orquestrar as transformaÃ§Ãµes
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ input/ â†’ Ficheiros de entrada (dados originais)
â”‚ â”‚ â”œâ”€â”€ *.csv / *.xml
â”‚ â”‚
â”‚ â””â”€â”€ output/ â†’ Ficheiros de saÃ­da gerados pelo processo ETL
â”‚ â””â”€â”€ *.json


---

## âš™ï¸ DescriÃ§Ã£o da SoluÃ§Ã£o

O objetivo deste trabalho foi desenvolver um **processo ETL (Extract, Transform, Load)** utilizando o **Pentaho Data Integration (Kettle)**.

### **Etapas Principais:**
1. **ExtraÃ§Ã£o:**  
   Leitura dos ficheiros de entrada localizados em `data/input/`.
2. **TransformaÃ§Ã£o:**  
   Limpeza, normalizaÃ§Ã£o e junÃ§Ã£o de dados conforme os requisitos definidos.  
   As transformaÃ§Ãµes (`.ktr`) encontram-se na pasta `dataint/`.
3. **Carregamento:**  
   Os resultados processados sÃ£o exportados para `data/output/`, nos formatos `.csv` ou `.xlsx`.

---

## ğŸš€ Como Executar a SoluÃ§Ã£o

### **PrÃ©-requisitos:**
- Ter instalado o **Pentaho Data Integration (PDI / Kettle)**  
- Ter o SQL server instalado, e a base de dados devidamente criada e configurada 

### **Passos:**
1. Abrir o **Spoon** (interface grÃ¡fica do Kettle).
2. Ir a **File â†’ Open** e abrir o job principal localizado em:
3. Verificar os caminhos relativos das entradas e saÃ­das:
- Entrada: `data/input/`
- SaÃ­da: `data/output/`
4. Executar o job clicando no botÃ£o **Run**.
5. ApÃ³s a execuÃ§Ã£o, verificar os ficheiros gerados em `data/output/`.

---

## ğŸ§© Ficheiros Principais
| Tipo | Nome / LocalizaÃ§Ã£o | DescriÃ§Ã£o |
|------|--------------------|------------|
| Job Principal | `dataint/ETL_Master.kjb` | Coordena todas as transformaÃ§Ãµes |
| TransformaÃ§Ã£o 1 | `dataint/Exames.ktr` | Limpeza e normalizaÃ§Ã£o dos dados dos exames |
| TransformaÃ§Ã£o 2 | `dataint/Consultas.ktr` | Limpeza e normalizaÃ§Ã£o dos dados das consultas |
| TransformaÃ§Ã£o 3 | `dataint/join.ktr` | JunÃ§Ã£o, envio de email e inserÃ§Ã£o dos dados na base de dados  |
| TransformaÃ§Ã£o 4 | `dataint/ExportarResultados.ktr` | ExportaÃ§Ã£o dos dados  |
| DocumentaÃ§Ã£o | `doc/27964_27983_doc.pdf` | ExplicaÃ§Ã£o detalhada do processo ETL e resultados obtidos |

---

## ğŸ§¾ GeraÃ§Ã£o dos Logs
Para gerar os logs da execuÃ§Ã£o do processo ETL, foram utilizados os seguintes comandos no Prompt de comando:

-cd C:\Users\JoÃ£o\Downloads\pdi-ce-10.2.0.0-222\data-integration

-Kitchen.bat /file:"C:\TP01_27964\dataint\ETL_Master.kjb" /level:Basic /logfile:"C:\TP01_27964\data\output\logs\novalog"

---

## ğŸ§ª Exemplos de ExecuÃ§Ã£o
ApÃ³s a execuÃ§Ã£o do job principal, serÃ£o gerados ficheiros de saÃ­da no diretÃ³rio:
Exemplo:5,Rita Santos,999888777,915678901,jpjbcmjg@gmail.com,2025-09-20,dermatologia,RaioX        ,Alterado,2025-09-21


---

## ğŸ§‘â€ğŸ’» ObservaÃ§Ãµes Finais
- Todos os caminhos de ficheiros foram configurados de forma **relativa**, facilitando a portabilidade do projeto.  
- Caso surjam erros de leitura ou escrita, verificar permissÃµes de pasta e encoding dos ficheiros de entrada.  
- O ficheiro PDF na pasta `doc/` contÃ©m explicaÃ§Ãµes detalhadas do processo, fluxos e prints das transformaÃ§Ãµes realizadas.

---

âœï¸ **Autor:** JoÃ£o Barbosa  
ğŸ“… **Data:** Outubro de 2025

